import torch
import json
import comfy.samplers


class WolfSigmasGet:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "model": ("MODEL",),
                "steps": ("INT", {"default": 20, "min": 1, "max": 10000}),
                "denoise": (
                    "FLOAT",
                    {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.01},
                ),
                "scheduler": (comfy.samplers.KSampler.SCHEDULERS,),
            }
        }

    RETURN_TYPES = ("SIGMAS", "STRING")
    RETURN_NAMES = ("SIGMAS", "sigmas_json")
    FUNCTION = "get_sigmas"
    CATEGORY = "sampling/sigmas_wolf"

    def get_sigmas(self, model, steps, denoise, scheduler):
        inner_model = model.model if hasattr(model, "model") else model

        if (
            not hasattr(inner_model, "model_sampling")
            or inner_model.model_sampling is None
        ):
            raise AttributeError(
                "The provided model's diffusion component (model.model) does not have a 'model_sampling' "
                "attribute, or it is None. This is required for sigma calculation."
            )

        # This is the object (e.g., KSampler, EPS, VE, or in this case, Chroma) that holds sigma_min, sigma_max, etc.
        model_sampling_config = inner_model.model_sampling

        if denoise == 0:
            if not hasattr(model_sampling_config, "sigma_max"):
                raise AttributeError(
                    f"The model_sampling_config ('{type(model_sampling_config).__name__}') does not have 'sigma_max' attribute."
                )
            sigmas = torch.FloatTensor([model_sampling_config.sigma_max])
        else:
            # Pass the model_sampling_config object to ComfyUI's sigma calculation function
            all_sigmas = comfy.samplers.calculate_sigmas(
                model_sampling_config, scheduler, steps
            )

            # Apply denoise by slicing
            num_denoise_steps = int(steps * denoise)
            if (
                num_denoise_steps == 0 and len(all_sigmas) > 0
            ):  # Handle cases where denoise is small but non-zero leading to 0 steps
                sigmas = torch.FloatTensor(
                    [all_sigmas[0]]
                )  # Or sigma_max, depends on desired behavior for near-zero denoise
            elif len(all_sigmas) < num_denoise_steps + 1:
                # This case should ideally not happen if calculate_sigmas gives enough sigmas for `steps`
                # but as a safeguard, or if steps * denoise is somehow > steps
                print(
                    f"Warning: Not enough sigmas generated by scheduler. Requested {num_denoise_steps + 1}, got {len(all_sigmas)}. Using all available."
                )
                sigmas = all_sigmas
            else:
                sigmas = all_sigmas[-(num_denoise_steps + 1) :]

        sigmas_list = sigmas.tolist()
        sigmas_json = json.dumps(sigmas_list, indent=2)
        return (sigmas, sigmas_json)
